{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing and Testing To Achieve a Functional Training Model -Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install datasets transformers[torch] evaluate accelerate rouge_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n",
    "import string\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "import os\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset('multi_news', trust_remote_code=True)\n",
    "\n",
    "# Convert to DataFrames\n",
    "ds_train = pd.DataFrame(ds['train'])\n",
    "ds_test = pd.DataFrame(ds['test'])\n",
    "\n",
    "# Display the first few rows\n",
    "print(ds_train.head())\n",
    "print(ds_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters using string.punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to preprocess in batches and save periodically\n",
    "def preprocess_and_save(dataset, save_path, batch_size=100):\n",
    "    total_rows = len(dataset)\n",
    "    for start in tqdm(range(0, total_rows, batch_size), desc=\"Processing batches\"):\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch = dataset.iloc[start:end].copy()  # Used .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "        # Preprocess the current batch\n",
    "        batch['document'] = batch['document'].map(preprocess_text)\n",
    "\n",
    "        # Save the processed batch\n",
    "        if start == 0:\n",
    "            batch.to_csv(save_path, index=False)\n",
    "        else:\n",
    "            batch.to_csv(save_path, mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"Processed and saved rows {start} to {end}\")\n",
    "\n",
    "# Check for existing preprocessed file\n",
    "save_path_train = 'preprocessed_train.csv'\n",
    "save_path_test = 'preprocessed_test.csv'\n",
    "\n",
    "# Preprocess training data\n",
    "if not os.path.exists(save_path_train):\n",
    "    preprocess_and_save(ds_train, save_path_train)\n",
    "\n",
    "# Preprocess test data\n",
    "if not os.path.exists(save_path_test):\n",
    "    preprocess_and_save(ds_test, save_path_test)\n",
    "\n",
    "print(\"Preprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "new_ds = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.read_csv(save_path_train)),\n",
    "    'test': Dataset.from_pandas(pd.read_csv(save_path_test))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Set maximum length explicitly 512\n",
    "max_length = 512\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['document'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_ds_train = new_ds['train'].map(tokenize_function, batched=True)\n",
    "tokenized_ds_test = new_ds['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "train_df = pd.DataFrame(tokenized_ds_train)\n",
    "test_df = pd.DataFrame(tokenized_ds_test)\n",
    "\n",
    "# Save to CSV\n",
    "train_df.to_csv('tokenized_ds_train.csv', index=False)\n",
    "test_df.to_csv('tokenized_ds_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and train BART model\n",
    "from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base').to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            \n",
    "    evaluation_strategy=\"epoch\",       \n",
    "    learning_rate=2e-5,                \n",
    "    per_device_train_batch_size=4,     \n",
    "    per_device_eval_batch_size=4,      \n",
    "    num_train_epochs=3,                \n",
    "    weight_decay=0.01,                 \n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_train,\n",
    "    eval_dataset=tokenized_ds_test,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
