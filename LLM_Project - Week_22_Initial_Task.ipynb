{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Project Initial Steps - Summarization Model using 'multi_news'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tfCiAlpCnSAq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Download NLTK Resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I have always had an interest in the news and events that go on around us, so I logically chose to create a model that would summarize news articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Dataset for Document Summarization\n",
        "ds = load_dataset('multi_news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View Available Splits\n",
        "print(ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View an Example Row from the Training Set\n",
        "print(ds['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Understanding Features of the Dataset\n",
        "print(ds['train'].features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrames\n",
        "ds_train = pd.DataFrame(ds['train'])\n",
        "ds_test = pd.DataFrame(ds['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the DataFrames\n",
        "print(ds_train.head())\n",
        "print(ds_test.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert DataFrames to Dataset Objects\n",
        "train = Dataset.from_pandas(ds_train)\n",
        "test = Dataset.from_pandas(ds_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DatasetDict\n",
        "new_ds = DatasetDict({\n",
        "    'train': train,\n",
        "    'test': test\n",
        "})\n",
        "\n",
        "# View the Resulting DatasetDict\n",
        "print(new_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Preprocessing on 'new_ds'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing Function\n",
        "stop_words = set(stopwords.words('english'))  # Load stopwords \n",
        "\n",
        "def preprocess_text(texts):\n",
        "    processed_texts = []\n",
        "    \n",
        "    for text in texts:\n",
        "        if text is None:  # Handle None values\n",
        "            continue\n",
        "        \n",
        "        # Check if the input is a list and join if necessary\n",
        "        if isinstance(text, list):\n",
        "            text = ' '.join(text)\n",
        "        \n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [word for word in tokens if word.isalnum()]\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        processed_texts.append(' '.join(tokens))\n",
        "        \n",
        "    return processed_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Preprocessing to the Dataset\n",
        "new_ds = new_ds.map(lambda x: {'preprocessed': preprocess_text(x['document'])}, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "document        0\n",
            "summary         0\n",
            "preprocessed    0\n",
            "dtype: int64\n",
            "document        0\n",
            "summary         0\n",
            "preprocessed    0\n",
            "dtype: int64\n",
            "Dataset({\n",
            "    features: ['document', 'summary', 'preprocessed'],\n",
            "    num_rows: 5\n",
            "})\n",
            "Dataset({\n",
            "    features: ['document', 'summary', 'preprocessed'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Check for null values in the preprocessed data\n",
        "train_df = new_ds['train'].to_pandas()\n",
        "test_df = new_ds['test'].to_pandas()\n",
        "\n",
        "print(train_df.isnull().sum())\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "# View the preprocessed data\n",
        "print(new_ds['train'].select(range(5)))  # Show first 5 processed samples\n",
        "print(new_ds['test'].select(range(5)))   # Show first 5 processed samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
